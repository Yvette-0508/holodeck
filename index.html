<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Flux + Mochi Video Generation System — Cloud-Only Blueprint</title>
    <meta
      name="description"
      content="A simple, cloud-only architecture blueprint for a Flux + Mochi video generation pipeline (from dev to production)."
    />
    <style>
      :root {
        --bg: #f4efe6;
        --paper: #ffffff;
        --text: #111111;
        --muted: rgba(17, 17, 17, 0.72);
        --faint: rgba(17, 17, 17, 0.46);
        --line: rgba(17, 17, 17, 0.16);
        --line-soft: rgba(17, 17, 17, 0.1);
        --accent: #111111;
        --code-bg: #f7f4ee;
      }

      html,
      body {
        height: 100%;
      }

      body {
        margin: 0;
        color: var(--text);
        background: var(--bg);
        font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
        line-height: 1.65;
        font-size: 16px;
        overflow-x: hidden;
        text-rendering: optimizeLegibility;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
      }

      .container {
        max-width: 1120px;
        margin: 0 auto;
        padding: 52px 22px 44px;
      }

      @media (max-width: 940px) {
        .container {
          padding: 36px 18px 36px;
        }
      }

      @media (max-width: 480px) {
        .container {
          padding: 28px 14px 32px;
        }
      }

      .layout {
        display: grid;
        grid-template-columns: 240px minmax(0, 1fr);
        gap: 46px;
        align-items: start;
      }

      @media (max-width: 940px) {
        .layout {
          grid-template-columns: 1fr;
          gap: 22px;
        }
      }

      .timeline {
        position: relative;
      }

      .timeline-inner {
        position: sticky;
        top: 26px;
        padding-top: 6px;
      }

      @media (max-width: 940px) {
        .timeline-inner {
          position: relative;
          top: auto;
          border-bottom: 1px solid var(--line-soft);
          padding-bottom: 16px;
          margin-bottom: 6px;
        }
      }

      .brand {
        font-size: 36px;
        line-height: 1.05;
        letter-spacing: 0.08em;
        text-transform: lowercase;
        margin-bottom: 18px;
        margin-left: -10px;
      }

      @media (max-width: 480px) {
        .brand {
          font-size: 30px;
          margin-left: 0;
        }
      }

      .kicker {
        font-size: 12px;
        letter-spacing: 0.22em;
        text-transform: uppercase;
        color: var(--faint);
        margin: 0 0 10px;
      }

      header {
        margin-bottom: 34px;
        max-width: 78ch;
      }

      .logo {
        margin: 0 0 18px;
        margin-left: -72px;
      }

      @media (max-width: 940px) {
        .logo {
          margin-left: 0;
        }
      }

      @media (max-width: 480px) {
        .logo {
          margin-left: 0;
        }
      }

      .logo img {
        display: block;
        width: min(320px, 78%);
        height: auto;
        margin: 0;
      }

      @media (max-width: 940px) {
        .logo img {
          width: min(360px, 90%);
        }
      }

      h1:empty {
        display: none;
      }

      h1 {
        font-size: 40px;
        line-height: 1.12;
        margin: 0 0 12px;
        letter-spacing: -0.02em;
      }

      .subtitle {
        margin: 0;
        color: var(--muted);
        max-width: 78ch;
        font-size: 16px;
      }

      .tagline {
        display: inline-block;
        color: var(--text);
        letter-spacing: 0.08em;
        text-transform: lowercase;
        font-weight: 600;
      }

      .timeline-title {
        font-size: 12px;
        letter-spacing: 0.22em;
        text-transform: uppercase;
        color: var(--faint);
        margin: 18px 0 10px;
      }

      .timeline-nav {
        position: relative;
        padding-left: 14px;
        border-left: 1px solid var(--line);
      }

      @media (max-width: 940px) {
        .timeline-nav {
          border-left: 0;
          padding-left: 0;
          display: flex;
          flex-wrap: wrap;
          gap: 10px 16px;
        }
        .timeline-nav a {
          grid-template-columns: 1fr;
          gap: 0;
          padding: 4px 0;
          font-size: 12px;
        }
        .dot {
          display: none;
        }
      }

      .timeline-nav a {
        display: grid;
        grid-template-columns: 14px 1fr;
        gap: 10px;
        align-items: start;
        padding: 7px 0;
        text-decoration: none;
        color: var(--muted);
        font-size: 13px;
      }

      .timeline-nav a:hover {
        color: var(--text);
      }

      .dot {
        width: 8px;
        height: 8px;
        border-radius: 999px;
        margin-top: 4px;
        border: 1px solid var(--line);
        background: var(--bg);
      }

      .content {
        max-width: 820px;
      }

      hr {
        border: 0;
        border-top: 1px solid var(--line-soft);
        margin: 26px 0;
      }

      section {
        padding: 0;
        margin: 34px 0;
        scroll-margin-top: 22px;
      }

      section:last-of-type {
        margin-bottom: 0;
      }

      h2 {
        margin: 0 0 10px;
        font-size: 16px;
        font-weight: 600;
        letter-spacing: 0.02em;
      }

      h3 {
        margin: 18px 0 10px;
        font-size: 16px;
        font-weight: 600;
      }

      p {
        margin: 10px 0;
        color: var(--muted);
      }

      ul,
      ol {
        margin: 10px 0 10px 20px;
        color: var(--muted);
      }

      li {
        margin: 6px 0;
      }

      strong {
        color: var(--text);
      }

      a {
        color: var(--text);
        text-decoration: underline;
        text-decoration-color: rgba(17, 17, 17, 0.25);
        text-underline-offset: 3px;
      }

      a:hover {
        text-decoration-color: rgba(17, 17, 17, 0.55);
      }

      table {
        width: 100%;
        border-collapse: collapse;
        margin: 14px 0;
        border-top: 1px solid var(--line);
        border-bottom: 1px solid var(--line);
      }

      @media (max-width: 640px) {
        table {
          display: block;
          overflow-x: auto;
          -webkit-overflow-scrolling: touch;
          min-width: 680px;
        }
        th,
        td {
          white-space: normal;
        }
      }

      @media (max-width: 640px) {
        pre {
          font-size: 13px;
        }
      }

      th,
      td {
        padding: 10px 10px;
        border-bottom: 1px solid var(--line-soft);
        vertical-align: top;
        text-align: left;
        color: var(--muted);
      }

      th {
        color: var(--text);
        font-weight: 600;
      }

      tr:last-child td {
        border-bottom: 0;
      }

      code {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono",
          "Courier New", monospace;
        font-size: 0.92em;
        background: rgba(255, 255, 255, 0.55);
        border: 1px solid var(--line-soft);
        padding: 2px 6px;
        border-radius: 8px;
        color: rgba(17, 17, 17, 0.9);
      }

      pre {
        background: rgba(255, 255, 255, 0.75);
        border: 1px solid var(--line-soft);
        border-radius: 12px;
        padding: 14px 14px;
        overflow: auto;
        margin: 14px 0;
      }

      pre code {
        background: transparent;
        border: 0;
        padding: 0;
        color: rgba(17, 17, 17, 0.92);
      }

      /* lightweight static "syntax" coloring (no JS) */
      pre .tok-kw {
        color: #6d28d9;
      }
      pre .tok-key {
        color: #0369a1;
      }
      pre .tok-str {
        color: #047857;
      }
      pre .tok-num {
        color: #be123c;
      }
      pre .tok-comment {
        color: rgba(17, 17, 17, 0.55);
      }
      pre .tok-op {
        color: #b45309;
      }

      .grid {
        display: grid;
        grid-template-columns: 1fr;
        gap: 16px;
      }

      @media (min-width: 940px) {
        .grid {
          grid-template-columns: 1fr 1fr;
        }
      }

      .note {
        padding: 12px 14px;
        border: 1px solid var(--line-soft);
        background: rgba(255, 255, 255, 0.45);
        border-radius: 12px;
        color: var(--muted);
      }

      footer {
        margin-top: 22px;
        color: var(--faint);
        font-size: 12px;
      }

      .log-entries {
        display: flex;
        flex-direction: column;
        gap: 12px;
      }

      .log-entry {
        padding: 14px 16px;
        background: rgba(255, 255, 255, 0.55);
        border: 1px solid var(--line-soft);
        border-radius: 10px;
      }

      .log-date {
        font-size: 12px;
        font-weight: 600;
        color: var(--faint);
        letter-spacing: 0.05em;
        margin-bottom: 6px;
      }

      .log-content {
        color: var(--muted);
        font-size: 15px;
        line-height: 1.5;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="layout">
        <aside class="timeline" aria-label="Timeline">
          <div class="timeline-inner">
            <div class="brand">holodeck</div>
            <div class="timeline-title">Contents</div>
            <nav class="timeline-nav" aria-label="Main sections">
              <a href="#why-cloud"><span class="dot"></span><span>Why this stack</span></a>
              <a href="#providers"><span class="dot"></span><span>Core model</span></a>
              <a href="#architecture"><span class="dot"></span><span>Architecture</span></a>
              <a href="#pipeline"><span class="dot"></span><span>Hardware</span></a>
              <a href="#models"><span class="dot"></span><span>Model inventory</span></a>
              <a href="#deployment"><span class="dot"></span><span>Workflow</span></a>
              <a href="#cost"><span class="dot"></span><span>Performance &amp; cost</span></a>
              <a href="#benchmarks"><span class="dot"></span><span>Future</span></a>
              <a href="#gallery"><span class="dot"></span><span>Gallery</span></a>
              <a href="#daily-log"><span class="dot"></span><span>Daily Log</span></a>
            </nav>
          </div>
        </aside>

        <main class="content">
          <header>
            <div class="logo" aria-label="Holodeck logo">
              <img src="logo.png" alt="Holodeck" />
            </div>
            <div class="kicker">Flux + Mochi</div>
            <h1></h1>
            <p class="subtitle">
              <span class="tagline">Infinite worlds, Summoned by voice</span>
            </p>
          </header>

          <hr />

          <section id="why-cloud">
            <h2>1) OSS end-to-end real-time video gen (Flux + Mochi)</h2>
            <p>
              I’m building an immersive, locally deployed pipeline that turns a novel (or a spoken
              narrative) into film-like video. The goal is privacy-first production and full control
              over model provenance.
            </p>
            <ul>
              <li><strong>Privacy</strong>: everything can run on your workstation.</li>
              <li><strong>Reproducibility</strong>: models + configs are explicit and versioned.</li>
              <li><strong>Quality</strong>: Flux for high-quality keyframes, Mochi for motion.</li>
            </ul>
          </section>

          <section id="providers">
            <h2>2) Core model: Mochi 1</h2>
            <p>
              <strong>Mochi 1</strong> is a widely used open-source DiT video model from Genmo AI.
              It’s known for strong physical motion: fluids, lighting changes, and particle motion
              feel consistent and “real.”
            </p>
            <ul>
              <li><strong>Industrial scale</strong>: ~10B parameters (not a toy model).</li>
              <li><strong>English-first training</strong>: strong at complex sci-fi descriptions.</li>
              <li><strong>Motion realism</strong>: coherent dynamics across frames.</li>
            </ul>
            <p>
              In practice, a <strong>dual RTX 4090 (48GB total VRAM)</strong> workstation is a great
              comfort zone for Mochi + decoding, especially with offload + tiling.
            </p>
          </section>

          <section id="architecture">
            <h2>3) Architecture overview</h2>
            <p>The pipeline is five layers, with a clean data flow:</p>
            <pre><code>Text / Audio input
      ↓
[Perception] Whisper Large-v3 (ASR)
      ↓
[Understanding] Llama-3.1-70B (scene split + prompt generation)
      ↓
[Static] Flux.1 Dev (4K keyframes)
      ↓
[Dynamic] Mochi 1 (physics-realistic video)
      ↓
[Post] Upscale + frame interpolation + encoding → 4K60 output</code></pre>
          </section>

          <section id="pipeline">
            <h2>4) Hardware configuration</h2>
            <h3>Local workstation (dual RTX 4090)</h3>
            <table>
              <thead>
                <tr>
                  <th>Part</th>
                  <th>Spec</th>
                  <th>Budget (USD)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>GPU</td>
                  <td>2× RTX 4090 24GB</td>
                  <td>Owned</td>
                </tr>
                <tr>
                  <td>CPU</td>
                  <td>Ryzen 9 7950X</td>
                  <td>~$550</td>
                </tr>
                <tr>
                  <td>RAM</td>
                  <td>128GB DDR5</td>
                  <td>~$400</td>
                </tr>
                <tr>
                  <td>NVMe</td>
                  <td>4TB Gen4</td>
                  <td>~$300</td>
                </tr>
                <tr>
                  <td>PSU</td>
                  <td>1600W (Platinum)</td>
                  <td>~$350</td>
                </tr>
                <tr>
                  <td><strong>Total</strong></td>
                  <td></td>
                  <td><strong>~$1,600</strong></td>
                </tr>
              </tbody>
            </table>

            <h3>VRAM allocation strategy</h3>
            <ul>
              <li><strong>GPU 0</strong>: Mochi inference + VAE decode.</li>
              <li><strong>GPU 1</strong>: Flux inference + Llama Int4.</li>
              <li><strong>CPU RAM</strong>: offload buffer for large models.</li>
            </ul>

            <h3>Cloud fallback (batch production)</h3>
            <table>
              <thead>
                <tr>
                  <th>Platform</th>
                  <th>Config</th>
                  <th>$/hour</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Vast.ai</td>
                  <td>2× 4090</td>
                  <td>$1.20</td>
                </tr>
                <tr>
                  <td>RunPod</td>
                  <td>2× A100</td>
                  <td>$3.50</td>
                </tr>
                <tr>
                  <td>Lambda</td>
                  <td>1× H100</td>
                  <td>$2.50</td>
                </tr>
              </tbody>
            </table>
            <p><strong>Simple strategy:</strong> develop locally, scale out in the cloud.</p>
          </section>

          <section id="models">
            <h2>5) Model inventory</h2>
            <table>
              <thead>
                <tr>
                  <th>Model</th>
                  <th>Size</th>
                  <th>Source</th>
                  <th>License</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Whisper Large-v3</td>
                  <td>~3GB</td>
                  <td>OpenAI</td>
                  <td>MIT</td>
                </tr>
                <tr>
                  <td>Llama-3.1-70B (Int4)</td>
                  <td>~35GB</td>
                  <td>Meta</td>
                  <td>Llama 3.1</td>
                </tr>
                <tr>
                  <td>Flux.1 Dev</td>
                  <td>~24GB</td>
                  <td>Black Forest Labs</td>
                  <td>Apache 2.0</td>
                </tr>
                <tr>
                  <td>Mochi 1 Preview</td>
                  <td>~40GB</td>
                  <td>Genmo</td>
                  <td>Apache 2.0</td>
                </tr>
                <tr>
                  <td>Real-ESRGAN</td>
                  <td>~64MB</td>
                  <td>xinntao</td>
                  <td>BSD</td>
                </tr>
                <tr>
                  <td>RIFE</td>
                  <td>~50MB</td>
                  <td>hzwer</td>
                  <td>MIT</td>
                </tr>
              </tbody>
            </table>
            <p><strong>Total</strong>: ~102GB. I keep ~200GB SSD free for caches and intermediate outputs.</p>
          </section>

          <section id="deployment">
            <h2>6) Workflow (what the system actually does)</h2>

            <h3>Stage 1 — Text preprocessing</h3>
            <p>
              A large LLM is used to produce a <strong>Scene Manifest JSON</strong>:
              scene boundaries, extracted visual elements, Flux prompts (composition + texture), and
              Mochi motion prompts (camera + physics).
            </p>

            <h3>Stage 2 — Keyframe generation (Flux.1 Dev)</h3>
            <p>Generate a 2048×1152 first frame per scene (optionally with ControlNet / IP-Adapter for consistency).</p>
            <ul>
              <li><strong>guidance_scale</strong>: 3.5</li>
              <li><strong>inference_steps</strong>: 50</li>
            </ul>

            <h3>Stage 3 — Video generation (Mochi 1)</h3>
            <p>
              Mochi takes the Flux keyframe and runs image-to-video inference:
              <strong>84 frames</strong> (~3.5s at 24fps) at 480×848, with guidance around 4.5.
            </p>
            <p><strong>VRAM optimization is critical</strong>:</p>
            <pre><code><span class="tok-comment"># Memory optimizations (key for dual 4090)</span>
<span class="tok-key">pipe</span>.<span class="tok-kw">enable_model_cpu_offload</span>()   <span class="tok-comment"># staged offload</span>
<span class="tok-key">pipe</span>.<span class="tok-kw">enable_vae_tiling</span>()          <span class="tok-comment"># tiled decode</span>
<span class="tok-key">torch</span>.<span class="tok-key">cuda</span>.<span class="tok-kw">empty_cache</span>()          <span class="tok-comment"># clean after each scene</span></code></pre>

            <h3>Stage 4 — Enhancement</h3>
            <ol>
              <li><strong>Real-ESRGAN 4×</strong>: 480p → 1920p</li>
              <li><strong>RIFE interpolation</strong>: 24fps → 60fps</li>
              <li><strong>FFmpeg (H.265)</strong>: CRF 18 for high quality</li>
            </ol>

            <h3>Stage 5 — Assembly</h3>
            <p>
              Stitch scenes by manifest order, add transitions, align audio, and export a final
              <strong>4K60</strong> deliverable.
            </p>
          </section>

          <section id="cost">
            <h2>7) Performance &amp; cost</h2>

            <h3>Per-scene timing (dual RTX 4090)</h3>
            <table>
              <thead>
                <tr>
                  <th>Step</th>
                  <th>Time</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Llama analysis</td>
                  <td>~10s</td>
                </tr>
                <tr>
                  <td>Flux keyframe</td>
                  <td>~45s</td>
                </tr>
                <tr>
                  <td>Mochi video</td>
                  <td>~180s</td>
                </tr>
                <tr>
                  <td>Post-processing</td>
                  <td>~60s</td>
                </tr>
                <tr>
                  <td><strong>Total per scene</strong></td>
                  <td><strong>~5 min</strong></td>
                </tr>
              </tbody>
            </table>
            <p>Output per scene: ~3.5 seconds of 4K60 video.</p>

            <h3>Project estimate (10-minute final video)</h3>
            <p>Assuming ~170 scenes:</p>
            <ul>
              <li><strong>Local</strong>: ~14 hours, roughly ~$5 electricity.</li>
              <li><strong>Cloud</strong>: ~8 hours, roughly ~$10–$30 GPU cost.</li>
            </ul>
          </section>

          <section id="benchmarks">
            <h2>8) Future: NVIDIA Cosmos</h2>
            <p>
              In the future, I also want to explore <strong>world models</strong> to push coherence,
              controllability, and longer-horizon storytelling.
            </p>
          </section>

          <section id="gallery">
            <h2>9) Gallery</h2>
            <p>
              View generated samples from the Flux + Mochi pipeline.
            </p>
            <a href="gallery.html" style="display: inline-block; padding: 10px 20px; background: var(--text); color: var(--bg); text-decoration: none; border-radius: 8px; font-weight: 600;">View Gallery →</a>
          </section>

          <section id="daily-log">
            <h2>10) Daily Log</h2>
            <div class="log-entries">
              <div class="log-entry">
                <div class="log-date">2026-01-22</div>
                <div class="log-content">Explored and run newly released WorldLab API.</div>
              </div>
              <div class="log-entry">
                <div class="log-date">2026-01-05</div>
                <div class="log-content">Generalize the pipeline into: user ask agent a question by clearly ask, not let agent guess by signals.</div>
              </div>
              <div class="log-entry">
                <div class="log-date">2026-01-04</div>
                <div class="log-content">Set up XGIMI projector and Google Play developer account. Tested Gemini Nano banana using voice input.</div>
              </div>
            </div>
          </section>

      <footer>
        Document v1.0 • Flux + Mochi • Cloud-only pipeline dev notes
      </footer>
        </main>
      </div>
    </div>
  </body>
</html>

